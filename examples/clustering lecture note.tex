%2multibyte Version: 5.50.0.2960 CodePage: 1251

\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{Codepage=1251}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Saturday, July 25, 2015 10:17:58}
%TCIDATA{LastRevised=Friday, March 17, 2017 15:13:44}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{Language=American English}
%TCIDATA{CSTFile=40 LaTeX article.cst}

\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent \textbf{#1.} }{\  \rule{0.5em}{0.5em}}

\begin{document}

\title{\bigskip Lecture 9: Standard errors, clustering and bootstrapping}
\author{S\o ren Leth-Petersen and Daniel le Maire \\
%EndAName
Department of Economics\\
University of Copenhagen}
\maketitle
\tableofcontents

\section{Inference for the linear model}

Most focus is typically on obtaining unbiased or consistent point estimates,
i.e. estimating $\beta $. However, to interpret this point estimate we need
that our estimated standard errors are unbiased to avoid misleading
inference.

In these notes, much of our focus will be on the case where observations are
grouped into clusters, where the error terms are correlated within clusters,
but uncorrelated across clusters. When making standard errors robust to
heteroscedasticity, it usually increases the standard errors, but not a lot.
When controlling for intra-cluster correlation, it is not unusual that
standard errors becomes several times larger.

We have already considered cluster-robust standard errors for the case of
individual fixed effects. In this case, the individual was treated as a
cluster to take account of (within-individual) serial correlated errors.
However, clustering is much more general problem, which arise in both panel
data and cross-sectional settings. Throughout these notes we will focus on
clustering in a cross-sectional data set. Furthermore, the focus in the
notes is on how to correct the standard errors of parameter estimates and
consequently we will not look into how to obtain more efficient parameter
estimates using feasible generalized least squares (FGLS).

We will begin these notes by deriving the standard errors under
homoscedasticity and heteroscedasticity. Next, we will consider two ways of
correcting the standard errors in case of clustering. Finally, we look at
how to use bootstrapping to estimate standard errors.\footnote{%
These notes mainly build on Angrist and Pischke (2009, chapter 8), Cameron
and Miller (2015). The final part on bootstrapping also draws on Horowitz
(2001) and Poi (2004).}

\section{Classical and heteroscedasticity robust standard errors}

Let $y_{i}=\mathbf{x}_{i}\beta +u_{i}$, where $y_{i}$ is the dependent
variable for individual $i=1,...,N$, $\mathbf{x}_{i}$ is a $1\times K$
vector of explanatory variables for individual $i$ (including a constant), $%
\beta $ is the $K\times 1$ vector of parameters, and $u_{i}$ is the error
term. Under the assumption of $E\left( u_{i}|\mathbf{x}_{i}\right) =0$ and
the usual rank condition, the OLS estimator is given by

\begin{eqnarray*}
\mathbf{\hat{\beta}} &=&\left[ \sum_{i=1}^{N}\mathbf{x}_{i}^{\prime }\mathbf{%
x}_{i}\right] ^{-1}\sum_{i=1}^{N}\mathbf{x}_{i}^{\prime }y_{i} \\
&=&\left( \mathbf{X}^{\prime }\mathbf{X}\right) ^{-1}\mathbf{X}^{\prime }%
\mathbf{y}
\end{eqnarray*}%
where $\mathbf{X}$ is the stacked $\mathbf{x}_{i}$ with dimension $N\times K$%
.

To derive the variance-covariance matrix, we insert $\mathbf{y=X\beta +u}$%
\begin{eqnarray*}
\mathbf{\hat{\beta}} &=&\left( \mathbf{X}^{\prime }\mathbf{X}\right) ^{-1}%
\mathbf{X}^{\prime }\left( \mathbf{X\beta }+\mathbf{u}\right) \Leftrightarrow
\\
\mathbf{\hat{\beta}}-\mathbf{\beta } &=&\left( \mathbf{X}^{\prime }\mathbf{X}%
\right) ^{-1}\mathbf{X}^{\prime }\mathbf{u}
\end{eqnarray*}%
Using this, we can write the variance of $\mathbf{\hat{\beta}}$ as%
\begin{eqnarray*}
Avar\left( \mathbf{\hat{\beta}}\right) &=&E\left[ \left( \mathbf{\hat{\beta}}%
-\mathbf{\beta }\right) \left( \mathbf{\hat{\beta}}-\mathbf{\beta }\right)
^{\prime }\right] \\
&=&E\left[ \left( \mathbf{X}^{\prime }\mathbf{X}\right) ^{-1}\mathbf{X}%
^{\prime }\mathbf{u}\left( \left( \mathbf{X}^{\prime }\mathbf{X}\right) ^{-1}%
\mathbf{X}^{\prime }\mathbf{u}\right) ^{\prime }\right] \\
&=&E\left[ \left( \mathbf{X}^{\prime }\mathbf{X}\right) ^{-1}\mathbf{X}%
^{\prime }\mathbf{uu}^{\prime }\mathbf{X}\left( \mathbf{X}^{\prime }\mathbf{X%
}\right) ^{-1}\right]
\end{eqnarray*}

Conditional on $\mathbf{X}$, we have%
\begin{equation}
Avar\left( \mathbf{\hat{\beta}}\right) =\left( \mathbf{X}^{\prime }\mathbf{X}%
\right) ^{-1}\mathbf{X}^{\prime }E\left( \mathbf{uu}^{\prime }|\mathbf{X}%
\right) \mathbf{X}\left( \mathbf{X}^{\prime }\mathbf{X}\right) ^{-1}
\label{Avar0}
\end{equation}%
Under homoscedasticity, we assume that\footnote{%
This is equivalent to assuming $E\left( u_{i}^{2}\mathbf{x}_{i}^{\prime }%
\mathbf{x}_{i}\right) =\sigma ^{2}E\left( \mathbf{x}_{i}^{\prime }\mathbf{x}%
_{i}\right) $ as in OLS.3.}%
\begin{equation}
E\left( \mathbf{uu}^{\prime }|\mathbf{X}\right) =\sigma ^{2}\mathbf{I}_{T}
\label{Omega homoscedastic}
\end{equation}%
Using this, equation $\left( \ref{Avar0}\right) $ gives%
\begin{eqnarray}
Avar\left( \mathbf{\hat{\beta}}\right) &=&\left( \mathbf{X}^{\prime }\mathbf{%
X}\right) ^{-1}\mathbf{X}^{\prime }\sigma ^{2}\mathbf{I}_{T}\mathbf{X}\left( 
\mathbf{X}^{\prime }\mathbf{X}\right) ^{-1}  \notag \\
&=&\sigma ^{2}\left( \mathbf{X}^{\prime }\mathbf{X}\right) ^{-1}
\label{Classical variance}
\end{eqnarray}%
where we estimate $\sigma ^{2}$ by $\hat{\sigma}^{2}=\frac{\sum_{i=1}^{N}%
\hat{u}_{i}^{2}}{N-K}$ where $\hat{u}_{i}=y_{i}-\mathbf{x}_{i}\mathbf{\hat{%
\beta}}$.\footnote{%
Since we deal with the asymptotic variance, $A\hat{v}ar\left( \hat{\beta}%
\right) $, the degrees of freedom is not really necessary.} The standard
errors of $\mathbf{\hat{\beta}}$ can be found as the square root of the
diagonal of the r.h.s. of equation $\left( \ref{Classical variance}\right) $%
. In these notes, we will refer to these standard errors as \emph{classical
standard errors}.

Heteroscedasticity implies that the variance of the error term is
non-constant, but independent. In this case,%
\begin{equation*}
E\left( \mathbf{uu}^{\prime }|\mathbf{X}\right) =\mathbf{\Omega }=\left[ 
\begin{array}{cccc}
\sigma _{1}^{2} & 0 & \cdots & 0 \\ 
0 & \sigma _{2}^{2} & \cdots & 0 \\ 
\vdots & \vdots & \ddots & 0 \\ 
0 & 0 & 0 & \sigma _{N}^{2}%
\end{array}%
\right]
\end{equation*}%
Using this assumption, White (1980a) showed that the following asymptotic
variance is robust to heteroscedasticity of unknown form%
\begin{eqnarray}
Avar\left( \hat{\beta}\right) &=&\left( \mathbf{X}^{\prime }\mathbf{X}%
\right) ^{-1}\mathbf{X}^{\prime }\mathbf{\Omega X}\left( \mathbf{X}^{\prime }%
\mathbf{X}\right) ^{-1}  \notag \\
&=&\left( \mathbf{X}^{\prime }\mathbf{X}\right) ^{-1}\left( \sum_{i=1}^{N}%
\mathbf{\sigma }_{i}^{2}\mathbf{x}_{i}^{\prime }\mathbf{x}_{i}\right) \left( 
\mathbf{X}^{\prime }\mathbf{X}\right) ^{-1}
\label{Heteroscedasticity robust}
\end{eqnarray}%
where we estimate $\sigma _{i}^{2}$ by $\hat{u}_{i}^{2}$.\footnote{%
It is actually the step where we replace $\sigma _{i}^{2}$ by $\hat{u}%
_{i}^{2}$ , which$\ $is the key result in White (1980a).}\textbf{\ }We will
refer to the standard errors which can be computed based on equation $\left( %
\ref{Heteroscedasticity robust}\right) $ as \emph{heteroscedasticity robust
standard errors}.

We should never rely solely on the classical standard errors. In fact, if
you view a regression as a linear approximation to the conditional
expectation function $E\left( y_{i}|\mathbf{x}_{i}\right) $ then, as White
(1980b) noticed, a true non-linear conditional expectation function would
imply heteroscedastic residuals even if you are prepared to assume that the
conditional variance $Var\left( y_{i}|\mathbf{x}_{i}\right) $ is constant.
To see this, write%
\begin{eqnarray*}
E\left[ \left( y_{i}-\mathbf{x}_{i}\mathbf{\beta }\right) ^{2}|\mathbf{x}_{i}%
\right] &=&E\left[ \left( y_{i}-E\left( y_{i}|\mathbf{x}_{i}\right) +E\left(
y_{i}|\mathbf{x}_{i}\right) -\mathbf{x}_{i}\mathbf{\beta }\right) ^{2}|%
\mathbf{x}_{i}\right] \\
&=&E\left[ \left. 
\begin{array}{c}
\left( y_{i}-E\left( y_{i}|\mathbf{x}_{i}\right) \right) ^{2}+\left( E\left(
y_{i}|\mathbf{x}_{i}\right) -\mathbf{x}_{i}\mathbf{\beta }\right) ^{2} \\ 
+2\left( y_{i}-E\left( y_{i}|\mathbf{x}_{i}\right) \right) \left( E\left(
y_{i}|\mathbf{x}_{i}\right) -\mathbf{x}_{i}\mathbf{\beta }\right)%
\end{array}%
\right\vert \mathbf{x}_{i}\right] \\
&=&E\left[ \left. 
\begin{array}{c}
\left( y_{i}-E\left( y_{i}|\mathbf{x}_{i}\right) \right) ^{2}+\left( E\left(
y_{i}|\mathbf{x}_{i}\right) -\mathbf{x}_{i}\mathbf{\beta }\right) ^{2} \\ 
+2\left( y_{i}-E\left( y_{i}|\mathbf{x}_{i}\right) \right) E\left( y_{i}|%
\mathbf{x}_{i}\right) -2\left( y_{i}-E\left( y_{i}|\mathbf{x}_{i}\right)
\right) \mathbf{x}_{i}\mathbf{\beta }%
\end{array}%
\right\vert \mathbf{x}_{i}\right] \\
&=&\left[ 
\begin{array}{c}
E\left[ \left( y_{i}-E\left( y_{i}|\mathbf{x}_{i}\right) \right) ^{2}|%
\mathbf{x}_{i}\right] +E\left[ \left( E\left( y_{i}|\mathbf{x}_{i}\right) -%
\mathbf{x}_{i}\mathbf{\beta }\right) ^{2}|\mathbf{x}_{i}\right] \\ 
+2\underset{=0}{\underbrace{E\left[ \left( y_{i}-E\left( y_{i}|\mathbf{x}%
_{i}\right) \right) E\left( y_{i}|\mathbf{x}_{i}\right) |\mathbf{x}_{i}%
\right] }}-2\underset{=0}{\underbrace{E\left[ \left( y_{i}-E\left( y_{i}|%
\mathbf{x}_{i}\right) \right) \mathbf{x}_{i}\mathbf{\beta |x}_{i}\right] }}%
\end{array}%
\right] \\
&=&Var\left( y_{i}|\mathbf{x}_{i}\right) +\left( E\left( y_{i}|\mathbf{x}%
_{i}\right) -\mathbf{x}_{i}\mathbf{\beta }\right) ^{2}
\end{eqnarray*}%
where we have used that $\left( y_{i}-E\left( y_{i}|\mathbf{x}_{i}\right)
\right) $ is mean independent of $\mathbf{x}_{i}$.\textbf{\ }Therefore, even
if you are willing to assume that $Var\left( y_{i}|\mathbf{x}_{i}\right) $
is constant, the residual variance will be varying if the true model is not
linear.

\begin{itemize}
\item Typically, the heteroscedasticity robust standard errors will be
larger than the classical standard errors.

\item Asymptotically, classical\ and heteroscedasticity robust\ standard
errors are correct under the appropriate assumptions, but both suffer from
finite sample bias, that will tend to make them too small in small samples.

\item It is primarily the heteroscedasticity robust standard errors that can
have a large bias. Chesher and Jewitt (1987) show that if there is not "too
much" heteroscedasticity, the heteroscedasticity robust standard errors will
even in "fairly large" samples be downward biased.

\item Part of the problem is that no correct degrees of freedom correction
exists for the heteroscedasticity robust standard errors.\footnote{%
MacKinnon and White (1985) discuss different type of corrections for the
residual variance estimator, for example $\sigma _{i}^{2}=\frac{N}{N-K}\hat{u%
}_{i}^{2}$, instead of $\sigma _{i}^{2}=\hat{u}_{i}^{2}$, which White
(1980a) used.} Recall that the degrees of freedom correction used under
homoscedasticity correct for the OLS residuals systematically being too
close to zero. The reason for this is that we have $K$ restrictions on the
OLS residuals, $\sum_{i=1}^{N}\mathbf{x}_{ij}\hat{u}_{i}=0$ for the $%
j=1,..,K-1$ explanatory variables and $\sum_{i=1}^{N}\hat{u}_{i}=0$, whereby
only $N\ -K$ residuals are free to vary.
\end{itemize}

Since no correct degrees of freedom correction exists for the
heteroscedasticity robust standard errors, it can easily be the case that
the classical OLS standard errors are largest. Below, we will discuss when
cluster robust standard errors are called for. When this is not required, we
should select the largest of the classical and heteroscedasticity robust
standard errors.

\section{Clustering}

When errors are positively correlated within a cluster (or a non-overlapping
group) then an additional observation in the cluster no longer provides a
completely independent piece of new information. Therefore, we cannot just
use the classical or heteroscedastic standard errors as they do not take the
intra-cluster correlation into account.

The standard example of clustering is a case where the left hand side
variable is an individual outcome and where at least one of the explanatory
variables is an aggregate variable. For example, if the outcome is the
individual probability of finding a job, one relevant aggregate variable
could be the local unemployment rate. Observations within a cluster can be
thought to be correlated as a result of an unobserved cluster effect. To be
more precise, an explanatory variable, which is aggregated, will not
necessarily lead to a clustering problem: If the inclusion of this variable
mops up all of the cluster-specific effect such that the remaining
error-term only has individual variation, we will not need to compute
cluster-robust standard errors. Clearly, this is highly unlikely to be the
case as this would imply that we have the right model in terms of functional
form for the aggregate level and that our aggregate explanatory variable is
not measured with error.

For example, imagine that you conduct an experiment where you want to
measure the effect of a new teaching program. Within $20$ different schools
(each with $4$ classes with $25$ students in each class) you randomize which
classes should have the program and not, so that $50$ percent of all classes
within each school are given the program. You then collect test scores for
all students in all the classes in the $20$ schools. Now the experimental
variation is at the class level and not at the student level. This means
that even if we have data on $2000$ students it is only the class level that
is useful for estimating the effect of the program. Actually it is as if we
only have $80$ observations corresponding to the number of classes. Using
the individual level data set with $2000$ observations we should therefore
cluster at the class level, i.e. allow covariances between students within
the same classes to be unrestricted.

\subsection{The Moulton factor}

We want to study the relationship between the classical standard errors and
clustered standard errors for the simplest case where we only have $\mathbf{x%
}$-variables which are constant within each cluster. The equation of
interest for member $m$ of the cluster is given by%
\begin{equation}
y_{gm}=\beta _{0}+\mathbf{x}_{gm}\mathbf{\beta }_{1}+v_{gm}  \label{ygm eq}
\end{equation}%
When stacking equation $\left( \ref{ygm eq}\right) $, we will make use of
the fact that all members of the cluster $g$ share the exact same values of
the explanatory variables as member $m$, that is $x_{gj}=x_{gm}$ for all $%
j=1,2,...,M_{g}$ where $M_{g}$ is the number of members (or observations) in
cluster $g$. Hence, when we stack $\mathbf{x}_{gm}$, we will use the $%
M_{g}\times K$ matrix $\mathbf{x}_{g}=\mathbf{j}_{M_{g}}\mathbf{x}_{gm}$
where $\mathbf{j}_{M_{g}}$ is a $M_{g}\times 1$ vector of ones. Stacking
observations within a cluster we can write%
\begin{equation*}
y_{g}=\beta _{0}+\mathbf{j}_{M_{g}}\mathbf{x}_{gm}\mathbf{\beta }_{1}+v_{g}
\end{equation*}

We assume that the residual has a random-effects group structure%
\begin{equation*}
v_{gm}=c_{g}+u_{gm}
\end{equation*}%
where $c_{g}$ is a random unobservable group component and $u_{gm}$ is a
mean-zero individual-level component. The implication of the random-effects
structure is that errors are equicorrelated within clusters. This is a
suitable assumption when the ordering of the observations within clusters is
irrelevant. When the cluster is a geographical unit this may be a reasonable
assumption.

Given this error-structure, the variance of the composite error-term is%
\begin{equation*}
\sigma _{v}^{2}=\sigma _{c}^{2}+\sigma _{u}^{2}
\end{equation*}%
The variance-covariance matrix is

\begin{equation*}
E\left( \mathbf{uu}^{\prime }|\mathbf{X}\right) =\mathbf{\Omega }=\left[ 
\begin{array}{cccc}
\mathbf{\Omega }_{1} & 0 & \cdots & 0 \\ 
0 & \mathbf{\Omega }_{2} & \cdots & 0 \\ 
\vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & \cdots & \mathbf{\Omega }_{G}%
\end{array}%
\right]
\end{equation*}%
where%
\begin{eqnarray}
\mathbf{\Omega }_{g} &=&\left[ 
\begin{array}{cccc}
\sigma _{c}^{2}+\sigma _{u}^{2} & \sigma _{c}^{2} & \cdots & \sigma _{c}^{2}
\\ 
\sigma _{c}^{2} & \sigma _{c}^{2}+\sigma _{u}^{2} & \cdots & \sigma _{c}^{2}
\\ 
\vdots & \vdots & \ddots & \vdots \\ 
\sigma _{c}^{2} & \sigma _{c}^{2} & \cdots & \sigma _{c}^{2}+\sigma _{u}^{2}%
\end{array}%
\right]  \notag \\
&=&\left[ \mathbf{I}_{M_{g}}\sigma _{u}^{2}+\mathbf{j}_{M_{g}}\mathbf{j}%
_{M_{g}}^{\prime }\sigma _{c}^{2}\right]
\label{Omega cluster equicorrelated}
\end{eqnarray}%
where $\mathbf{I}_{M_{g}}$ is a $M_{g}\times M_{g}$ identity matrix and $%
\mathbf{j}_{M_{g}}$ is a $M_{g}\times 1$ vector of ones.

We will continue by computing the variances under the assumption of
non-stochastic $x$'s. Replacing the $\mathbf{\Omega }=E\left( \mathbf{uu}%
^{\prime }|\mathbf{X}\right) $, the variance is given by%
\begin{equation}
Avar\left( \mathbf{\hat{\beta}}\right) =\left( \mathbf{X}^{\prime }\mathbf{X}%
\right) ^{-1}\mathbf{X}^{\prime }\mathbf{\Omega X}\left( \mathbf{X}^{\prime }%
\mathbf{X}\right) ^{-1}  \label{variance general}
\end{equation}%
First consider the matrix $\mathbf{X}^{\prime }\mathbf{X}$%
\begin{eqnarray}
\mathbf{X}^{\prime }\mathbf{X} &\mathbf{=}&\sum_{g=1}^{G}\left( \mathbf{j}%
_{M_{g}}\mathbf{x}_{gm}\right) ^{\prime }\mathbf{j}_{M_{g}}\mathbf{x}_{gm} 
\notag \\
&=&\sum_{g=1}^{G}\mathbf{x}_{gm}^{\prime }\mathbf{j}_{M_{g}}^{\prime }%
\mathbf{j}_{M_{g}}\mathbf{x}_{gm}  \notag \\
&=&\sum_{g=1}^{G}M_{g}\mathbf{x}_{gm}^{\prime }\mathbf{x}_{gm}  \label{X'X}
\end{eqnarray}

Next, consider the matrix $\mathbf{X}^{\prime }\mathbf{\Omega X}$%
\begin{eqnarray}
\mathbf{X}^{\prime }\mathbf{\Omega X} &\mathbf{=}&\sum_{g=1}^{G}\mathbf{x}%
_{gm}^{\prime }\mathbf{j}_{M_{g}}^{\prime }\mathbf{\Omega }_{g}\mathbf{j}%
_{M_{g}}\mathbf{x}_{gm}  \notag \\
&=&\sum_{g=1}^{G}\mathbf{x}_{gm}^{\prime }\mathbf{j}_{M_{g}}^{\prime }\left[ 
\mathbf{I}_{M_{g}}\sigma _{u}^{2}+\mathbf{j}_{M_{g}}\mathbf{j}%
_{M_{g}}^{\prime }\sigma _{c}^{2}\right] \mathbf{j}_{M_{g}}\mathbf{x}_{gm} 
\notag \\
&=&\sum_{g=1}^{G}\left( \mathbf{x}_{gm}^{\prime }\mathbf{j}_{M_{g}}^{\prime }%
\mathbf{j}_{M_{g}}\sigma _{u}^{2}\mathbf{x}_{gm}+\mathbf{x}_{gm}^{\prime }%
\mathbf{j}_{M_{g}}^{\prime }\mathbf{j}_{M_{g}}\mathbf{j}_{M_{g}}^{\prime
}\sigma _{c}^{2}\mathbf{j}_{M_{g}}\mathbf{x}_{gm}\right)  \notag \\
&=&\sum_{g=1}^{G}M_{g}\left( \sigma _{u}^{2}+M_{g}\sigma _{c}^{2}\right) 
\mathbf{x}_{gm}^{\prime }\mathbf{x}_{gm}  \notag \\
&=&\sigma _{v}^{2}\sum_{g=1}^{G}M_{g}\left( \frac{\sigma _{u}^{2}}{\sigma
_{c}^{2}+\sigma _{u}^{2}}+M_{g}\frac{\sigma _{c}^{2}}{\sigma _{c}^{2}+\sigma
_{u}^{2}}\right) \mathbf{x}_{gm}^{\prime }\mathbf{x}_{gm}  \notag \\
&=&\sigma _{v}^{2}\sum_{g=1}^{G}M_{g}\left( 1+\left( M_{g}-1\right) \frac{%
\sigma _{c}^{2}}{\sigma _{c}^{2}+\sigma _{u}^{2}}\right) \mathbf{x}%
_{gm}^{\prime }\mathbf{x}_{gm}  \notag \\
&=&\sigma _{v}^{2}\sum_{g=1}^{G}M_{g}\left( 1+\left( M_{g}-1\right) \rho
_{v}\right) \mathbf{x}_{gm}^{\prime }\mathbf{x}_{gm}  \label{X'OMEGAX}
\end{eqnarray}%
where $\rho _{v}\equiv \frac{\sigma _{c}^{2}}{\sigma _{c}^{2}+\sigma _{u}^{2}%
}$ is called the intra-class correlation coefficient.

Inserting equations $\left( \ref{X'X}\right) $ and $\left( \ref{X'OMEGAX}%
\right) $ in the variance formula $\left( \ref{variance general}\right) $,
we obtain%
\begin{equation*}
Avar\left( \mathbf{\hat{\beta}}_{1}\right) =\sigma _{v}^{2}\left(
\sum_{g=1}^{G}M_{g}\mathbf{x}_{gm}^{\prime }\mathbf{x}_{gm}\right)
^{-1}\sum_{g=1}^{G}M_{g}\left( 1+\left( M_{g}-1\right) \rho _{v}\right) 
\mathbf{x}_{gm}^{\prime }\mathbf{x}_{gm}\left( \sum_{g=1}^{G}M_{g}\mathbf{x}%
_{gm}^{\prime }\mathbf{x}_{gm}\right) ^{-1}
\end{equation*}%
Now, suppose the cluster sizes are equal, i.e. $M_{g}=M$. Then, we have that%
\begin{equation}
Avar\left( \mathbf{\hat{\beta}}_{1}\right) =\left( 1+\left( M-1\right) \rho
_{v}\right) \sigma _{v}^{2}\left( \sum_{g=1}^{G}M\mathbf{x}_{gm}^{\prime }%
\mathbf{x}_{gm}\right) ^{-1}  \label{Moulton Avar cluster}
\end{equation}%
Comparing this cluster-robust variance with the classical OLS variance%
\begin{equation}
Avar\left( \mathbf{\hat{\beta}}_{1}\right) =\sigma _{v}^{2}\left(
\sum_{g=1}^{G}M_{g}\mathbf{x}_{gm}^{\prime }\mathbf{x}_{gm}\right) ^{-1}
\label{Moulton Avar classical}
\end{equation}%
we can compute the ratio between the two variances in equations $\left( \ref%
{Moulton Avar cluster}\right) $ and $\left( \ref{Moulton Avar classical}%
\right) $\footnote{%
Where we for simplicity set $M_{g}=M$ for all $g$.} as $1+\left( M-1\right)
\rho _{v}$. The square-root of this is called the Moulton factor after
Moulton (1986) since%
\begin{equation*}
se_{cluster}\left( \mathbf{\hat{\beta}}_{1}\right) =se_{classical}\left( 
\mathbf{\hat{\beta}}_{1}\right) \sqrt{1+\left( M-1\right) \rho _{v}}
\end{equation*}

\begin{itemize}
\item The Moulton factor tells us how much the precision of the standard
errors is overstated when ignoring the intra-cluster correlation.

\item For a fixed number of observations, $N$, a higher $M$ implies fewer
clusters and since there is no dependence between clusters and only
dependence within each cluster, this increases the overall dependence in the
data. In other words, with the classical standard errors, each additional
observation adds new and independent information, but in reality additional
observations within a group does not add so much information.

\item A higher intra-class correlation $\rho _{v}$ increases the Moulton
factor since this implies that additional observations within a group
provide less new information and this is not acknowledged using the
classical standard errors.
\end{itemize}

The Moulton factor for the case where the covariate $x_{gm}$ varies at the
individual level and the cluster size also varies between clusters is given
by%
\begin{equation}
se_{cluster}\left( \mathbf{\hat{\beta}}_{1}\right) =se_{classical}\left( 
\mathbf{\hat{\beta}}_{1}\right) \sqrt{1+\left( \frac{Var\left( M_{g}\right) 
}{\bar{M}}+\bar{M}-1\right) \rho _{x}\rho _{v}}
\label{Standard errors and general Moulton factor}
\end{equation}%
where $\bar{M}\equiv \frac{1}{G}\sum_{g=1}^{G}M_{g}$ and $\rho _{x}\equiv 
\frac{\sum_{g=1}^{G}\sum_{m=1}^{M_{g}}\sum_{j=1}^{M_{g}}1\left[ m\neq j%
\right] \left( x_{gm}-\bar{x}\right) \left( x_{gj}-\bar{x}\right) }{V\left(
x_{gm}\right) \sum_{g=1}^{G}M_{g}\left( M_{g}-1\right) }$ is the
intra-cluster correlation of $x_{gm}$.

Notice that if $x_{gm}$ is uncorrelated within clusters, there is no
clustering problem and the Moulton factor is $1$. Hence, clustering is
likely to be a larger problem with covariates being fixed within clusters.

\subsection{Cluster-corrected standard errors}

Besides using the Moulton factor to correct estimated standard errors, it is
also possible to compute cluster-robust standard errors directly. With the
cluster structure, errors are only correlated within-cluster and not across
clusters. Hence, the errors have a block-diagonal structure. The
within-cluster variance-covariance matrix of the residuals is\footnote{%
To ease the notation, we have avoided the subscript $g$ of $M_{g}$ in the
last row and column in the matrix. However, we should think of $\Omega _{g}$
being $M_{g}\times M_{g}$.}%
\begin{equation}
\mathbf{\Omega }_{g}=E\left( \mathbf{u}_{g}\mathbf{u}_{g}^{\prime }|\mathbf{x%
}_{g}\right) =\left[ 
\begin{array}{cccc}
\sigma _{g11}^{2} & \sigma _{g12}^{2} & \cdots & \sigma _{g1M}^{2} \\ 
\sigma _{g21}^{2} & \sigma _{g22}^{2} & \cdots & \sigma _{g2M}^{2} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\sigma _{gM1}^{2} & \sigma _{gM2}^{2} & \cdots & \sigma _{gMM}^{2}%
\end{array}%
\right]  \label{Omega cluster}
\end{equation}%
Notice that within each cluster no restrictions are imposed on the type of
correlation. This is in contrast to the variance-covariance matrix of the
residuals in equation $\left( \ref{Omega cluster equicorrelated}\right) $,
where the restriction of equicorrelated errors was imposed.

Inserting this in equation $\left( \ref{Avar0}\right) $, we arrive at

\begin{equation}
Avar\left( \mathbf{\hat{\beta}}\right) =\left( \mathbf{X}^{\prime }\mathbf{X}%
\right) ^{-1}\left( \sum_{g=1}^{G}\mathbf{x}_{g}^{\prime }E\left( \mathbf{u}%
_{g}\mathbf{u}_{g}^{\prime }|\mathbf{x}_{g}\right) \mathbf{x}_{g}\right)
\left( \mathbf{X}^{\prime }\mathbf{X}\right) ^{-1}
\label{Cluster robust variance}
\end{equation}

To estimate this variance, $E\left( \mathbf{u}_{g}\mathbf{u}_{g}^{\prime }|%
\mathbf{x}_{g}\right) $ in equation $\left( \ref{Cluster robust variance}%
\right) $ is replaced by $\mathbf{\hat{u}}_{g}\mathbf{\hat{u}}_{g}$, where $%
\mathbf{\hat{u}}_{g}=\mathbf{y}_{g}-\mathbf{X}_{g}\mathbf{\hat{\beta}}$. At
first sight, this variance looks quite similar to the heteroscedasticity
robust variance in equation $\left( \ref{Heteroscedasticity robust}\right) $%
. However, whereas we are summing over individual observations in the latter
case, we are summing over whole clusters in equation $\left( \ref{Cluster
robust variance}\right) $. This difference reflects that whereas
observations are independent in the case of heteroscedasticity such that the
variance-covariance matrix of the residuals is a diagonal matrix,
observations within a cluster are dependent as is also apparent from the
variance-covariance matrix in equation $\left( \ref{Omega cluster}\right) $.

This is a consistent cluster robust variance estimator if%
\begin{equation*}
G^{-1}\sum_{g=1}^{G}\mathbf{x}_{g}^{\prime }\mathbf{\hat{u}}_{g}\mathbf{\hat{%
u}}_{g}^{\prime }\mathbf{x}_{g}-G^{-1}\sum_{g=1}^{G}E\left( \mathbf{x}%
_{g}^{\prime }\mathbf{u}_{g}\mathbf{u}_{g}^{\prime }\mathbf{x}_{g}\right) 
\overset{p}{\rightarrow }\mathbf{0}\text{ as }G\rightarrow \infty
\end{equation*}%
Why is the cluster robust variance estimator only consistent as $%
G\rightarrow \infty $, and not $N\rightarrow \infty $? To begin with notice
that residual variance covariance matrix $\mathbf{uu}^{\prime }$ in equation 
$\left( \ref{Omega homoscedastic}\right) $ has dimension $N\times N$. Under
homoscedasticity there is no correlation between $u_{i}$ and $u_{j}$ for $%
i\neq j$ and the matrix $\mathbf{uu}^{\prime }$ is a diagonal matrix.
Furthermore, since $E\left( u_{i}^{2}\right) $ is the same for all $i$, we
can estimate this by taking the average of $u_{i}^{2}$ for $i=1,...,N$.
Therefore, we are averaging over $N$ observations and in order to use the
Law of Large Numbers we let $N\rightarrow \infty $. This way, the classical
standard errors are consistent as $N\rightarrow \infty $. Compare this with
equation $\left( \ref{Cluster robust variance}\right) $ where $\mathbf{u}_{g}%
\mathbf{u}_{g}^{\prime }$ is $M_{g}\times M_{g}$ and where we cannot set any
cells to zero within a cluster. Replacing $E\left( \mathbf{u}_{g}\mathbf{u}%
_{g}^{\prime }\right) $ with $\mathbf{\hat{u}}_{g}\mathbf{\hat{u}}%
_{g}^{\prime }$ is likely to be a very poor estimate. However, since we have 
$G$ clusters, we can average this matrix over the $G$ clusters.\footnote{%
This is easiest to intuitively understand if we consider the case of equal
cluster size, that is $M_{g}=M$. The variance estimator in equation $\left( %
\ref{Cluster robust variance}\right) $ was also first derived by White
(1984) for the case of \ balanced clusters and then subsequently by Liang
and Zeger for the unbalanced case.} To use the Law of Large Numbers for
cluster standard errors, we need that $G\rightarrow \infty $.

The cluster-robust variance estimator in equation $\left( \ref{Cluster
robust variance}\right) $ and the Moulton factor in equation $\left( \ref%
{Standard errors and general Moulton factor}\right) $ are both asymptotic
results for $G\rightarrow \infty $. The usual rule of thumb is that these
formulas work with $50$ or more clusters. With a small number of clusters ($%
G<50$) or very unbalanced cluster sizes $M_{g}$, inference using
cluster-robust errors can be incorrect and worse than the classical or
heteroscedasticity robust standard errors.

\begin{itemize}
\item As with heteroscedasticity-robust standard errors, there exist no
correction, which will make cluster robust standard errors unbiased. In
practice the finite-sample adjustment factor $\frac{N-1}{N-K}\frac{G}{G-1}$
is often used (as it is implemented in STATA). In SAS, the finite sample
correction is $\frac{G}{G-1}$ and this simpler correction term is also used
in STATA for non-linear models.

\item A second general problem with few clusters also arise:

\begin{itemize}
\item Recall, that with OLS under homoscedasticity, we need to use the $t$%
-distribution rather than the standard normal distribution in small samples
since we replace the variance of the error term with the estimated
counterpart when estimating the standard errors of the $\beta $-estimates.
If we instead use the standard normal distribution, we will over-reject when
we use Wald tests.

\item With cluster-robust standard errors, it turns out that using the $t$%
-distribution will not be enough to fix the standard errors. Even when
bias-correcting the cluster-robust standard errors and using a $t$%
-distribution with $G-1$ degrees of freedom for Wald tests, we will with few
clusters over-reject the null hypothesis of $\beta =0$.

\item Nevertheless, it is still better than using the critical values from
the $t$-distribution than from the standard normal distribution when we have
few clusters.
\end{itemize}
\end{itemize}

\subsection{What to cluster over?}

It is not always obvious what to cluster over. In the example above with
randomization on class level, one could fear that there is a school-level
component in the errors such that clustering on school level rather than
class-level is called for. However, as a practical rule of thumb one should
cluster at the level of experimentation.

\begin{itemize}
\item Clearly, we should only define clusters broad enough to allow for the
error correlation we expect, but not define the groups too broad such that
we just let zero correlated observations into a cluster.

\item Furthermore, we should not define clusters so large that there are too
few clusters.

\item Sometimes aggregate explanatory variables can guide us such that we at
least cluster over the same level as the aggregate variable is defined by.
For example, Browning, G\o rtz and Leth-Petersen (2013) study the effect of
housing prices on consumption. Since all houses are not sold every year,
they cannot use the individual house price and instead they use the average
housing price at municipality level to estimate the effect of housing prices
on consumption. Since the average housing price on municipality level is an
aggregated variable, they cluster on municipality level.

\item Another rule of thumb is to progressively increase the cluster size
and stop whenever there is relatively little change in the estimated
standard errors.

\item It is sometimes the case that clusters are non-nested, for example,
industries and municipalities. In this case, it will be wrong to cluster
over the intersection, i.e. a particular industry in a particular
municipality should not be a cluster. Instead, we should allow for $E\left(
u_{i}u_{j}|x_{i},x_{j}\right) \neq 0$ whenever individuals $i$ and $j$ are
in the same cluster either in terms of industry or in terms of municipality.%
\footnote{%
The reader is referred to Cameron, Gelbach and Miller (2011) and Thompson
(2011) for the theory of two-way clustering.}
\end{itemize}

\section{Bootstrapping}

Bootstrapping is an easy way to obtain standard errors, confidence intervals
or critical values for test statistics and p-values, but why and when it
works is much more difficult.

There are two ways of obtaining standard errors for an estimate:

\begin{itemize}
\item Asymptotics.

\item Bootstrapping.
\end{itemize}

Bootstrapping is a method for estimating the distribution of an estimator or
test statistic by re-sampling one's data, that is treating the data (the
sample) as if it was the population.

\subsection{Why the bootstrap?}

\begin{itemize}
\item Sometimes it is difficult to derive asymptotic distribution of an
estimator or statistic, e.g. two-step estimators.

\item Asymptotic results may be very inaccurate in finite samples and it is
very difficult to derive small sample properties of estimators.

\item Bootstrap approximates the distribution of an estimator or test
statistic well and often more accurate in finite samples.
\end{itemize}

\subsection{Why not the bootstrap?}

\begin{itemize}
\item Computational expensive - model has to be estimated many times.

\item The numerical performance of the bootstrap may be poor, when
estimators whose asymptotic covariance matrices are "nearly singular", as
with instrumental variable estimation with many weak instruments.

\item Bootstrap is sometimes biased and should not be used blindly or
uncritically.
\end{itemize}

\subsection{Why is the method called the bootstrap?}

Baron M\"{u}nchausen is both a historical and literary character. The
historical character lived in Germany in the 18th century. When he was
young, he joined the Russian army where he became captain. He retired at the
age of 30 and lived the rest of his life at his manor. He is known to have
told witty and exaggerated stories mainly about his time in the Russian
army. In one of these stories, he falls into a swamp and cannot get up.
According to the story he pulled himself up by the bootstrap.\footnote{%
There actually seems to be disagreement whether he pulled himself up by the
bootstrap or the hair.} The point of this story is that he got himself out
of the problem by using existing resources. The bootstrap uses existing data
to generate the (unknown) population distribution.

\subsection{The nonparametric bootstrap}

There exist different types of bootstrap estimators. We will begin by
considering the simplest, the nonparametric bootstrap (sometimes also called
the pairs bootstrap). The nonparametric bootstrap is a very general
bootstrap, which can be applied to a wide range of models including
non-linear models. However, other bootstrap methods generally provide a
better approximation than the nonparametric bootstrap. Furthermore, we
should notice that with the nonparametric bootstrap we assume that there is
no cluster-correlation. Therefore, we will briefly consider what to do in
case of clustered data.

The algorithm for the nonparametric bootstrap is:

\begin{enumerate}
\item Estimate model on original sample to obtain the statistic $T_{N}$

\item Draw with replacement pairs $\left( y_{i},\mathbf{x}_{i}\right) $
until a bootstrap sample of size $N$ is reached, that is $\left( y_{1}^{\ast
},\mathbf{x}_{1}^{\ast }\right) ,\left( y_{2}^{\ast },\mathbf{x}_{2}^{\ast
}\right) ,...\left( y_{N}^{\ast },\mathbf{x}_{N}^{\ast }\right) $.

\item Use the bootstrap sample to obtain an estimate $T_{N,b}^{\ast }$.

\item Repeat 2)-3) many times to obtain a sequence of bootstrap estimates, $%
T_{N,b}^{\ast }$, $b=1,..,B$.

\item Calculate for example the standard deviation of the $B$ values of $%
T_{N,b}^{\ast }$.
\end{enumerate}

\bigskip

\emph{Statistics}

\begin{itemize}
\item The sample variance (from which we can compute the standard errors)%
\begin{equation*}
\frac{1}{B-1}\sum_{b=1}^{B}\left[ T_{N,b}^{\ast }-\bar{T}_{N}^{\ast }\right] %
\left[ T_{N,b}^{\ast }-\bar{T}_{N}^{\ast }\right] ^{\prime }
\end{equation*}%
where $\bar{T}_{N}^{\ast }$ is the mean of the $B$ bootstrap statistics $%
T_{N,b}^{\ast }$.

\item $95\%$ confidence intervals can be obtain by finding $0.025$ and $%
0.975 $ percentiles in the bootstrap distribution of $T_{N}^{\ast }$. If we
use the percentiles in the tails of the distribution, we would need more
bootstrap replications than when computing the variance.
\end{itemize}

\subsection{Consistency of the bootstrap}

We use $\left\{ X_{i}:1,...,N\right\} $ as notation for the data, where $%
X_{i}$ typically is a vector $\left( y_{i},\mathbf{x}_{i}\right) $. The data
are assumed to be iid draws from the population cdf $F=\Pr \left( X\leq
x\right) $.

The statistic of interest is a function of $X_{1},...,X_{N}$ and is denoted $%
T_{N}=T_{N}\left( X_{1},...,X_{N}\right) $. This statistic has its own exact
finite-sample distribution, which depends on $N$. Denote the finite sample
distribution by $G_{N}=\Pr \left( T_{N}\leq t\right) =G_{N}\left( t,F\right) 
$. Ideally, the best thing to do would be to use this exact finite sample
distribution for inference, but this is in general infeasible. Hence, for
inference the problem is always to find a good approximation to $G_{N}$.
When applying conventional asymptotic theory we let $N\rightarrow \infty $
and use the asymptotic distribution $G_{\infty }=G_{\infty }\left(
t,F\right) $. In other words,%
\begin{equation*}
T_{N}\overset{d}{\rightarrow }T_{\infty }
\end{equation*}

The approach of bootstrap is quite different from this. Instead of replacing 
$G_{N}$ with $G_{\infty }$, bootstrapping replaces the population cdf $F$ by
a consistent estimator $\hat{F}$ of $F$. In other words, we use the
bootstrap to approximate $G_{N}\left( \cdot ,\hat{F}\right) $. There are
several ways of constructing $\hat{F}$. For the nonparametric bootstrap $%
\hat{F}$ is the empirical distribution function%
\begin{equation*}
\hat{F}\left( x\right) =\frac{1}{N}\dsum \limits_{i=1}^{N}1\left( X_{i}\leq
x\right)
\end{equation*}%
where the function $1\left( \cdot \right) $ is an indicator function. An
alternative is the parametric bootstrap, where $\hat{F}\left( x\right) =F_{%
\hat{\theta}}\left( x\right) $, where $F\left( \cdot \right) =F_{\theta
}\left( \cdot \right) $. Hence, if $F\left( \cdot \right) =N\left( \mu
,\sigma ^{2}\right) $ we can use $\hat{F}\left( x\right) =N\left( \hat{\mu},%
\hat{\sigma}^{2}\right) $.

The bootstrapped statistic is%
\begin{equation*}
T_{N}^{\ast }=T_{N}\left( X_{1}^{\ast },...,X_{N}^{\ast }\right)
\end{equation*}%
where $T_{N}^{\ast }$ is a random variable and where $X_{1}^{\ast
},...,X_{N}^{\ast }$ are drawn randomly with replacement. The empirical
cumulative distribution function of the bootstrapped statistics $%
T_{N,1}^{\ast },...,T_{N,B}^{\ast }$ is%
\begin{equation}
\hat{G}_{N}\left( t,\hat{F}\right) =P\left( T_{N,b}^{\ast }\leq t\right)
\label{GNhat}
\end{equation}

Consistency of the bootstrap implies that $\left \Vert G_{N}\left( t,\hat{F}%
\right) -G_{\infty }\left( t,F\right) \right \Vert \overset{p}{\rightarrow }%
0 $ as $N\rightarrow \infty $ uniformly over all $t$. To show that this is
the case we can split the l.h.s. in two%
\begin{equation}
\left \Vert G_{N}\left( t,\hat{F}\right) -G_{\infty }\left( t,F\right)
\right \Vert \leq \left \Vert G_{N}\left( t,\hat{F}\right) -G_{N}\left(
t,F\right) \right \Vert +\left \Vert G_{N}\left( t,F\right) -G_{\infty
}\left( t,F\right) \right \Vert  \label{consistency}
\end{equation}

\begin{itemize}
\item Consider the first part of the r.h.s. Since $\hat{F}$ is a random
variable, $G_{N}\left( t,\hat{F}\right) $ is also a random variable and we
must refer to some kind of probabilistic convergence%
\begin{eqnarray*}
\left \Vert G_{N}\left( t,\hat{F}\right) -G_{N}\left( t,F\right) \right
\Vert &=&\left \Vert P^{\ast }\left( T_{N}^{\ast }\leq t\right) -P\left(
T_{N}\leq t\right) \right \Vert \\
&=&\underset{t}{\sup }\left \vert P^{\ast }\left( T_{N}^{\ast }\leq t\right)
-P\left( T_{N}\leq t\right) \right \vert \overset{p}{\rightarrow }0
\end{eqnarray*}%
where we use the Kolmogorov-Smirnov distance, which is the sup-norm. When $%
\hat{F}\left( x\right) \rightarrow F\left( x\right) $ and when $G_{N}\left(
t,F\right) $ is continuous in $F$, $G_{N}\left( t,\hat{F}\right)
\longrightarrow G_{N}\left( t,F\right) $.

\item The last part deals with the uniform convergence in distribution of $%
T_{N}$ to $T_{\infty }$. If the limiting distribution $G_{\infty }\left(
t,F\right) $ is continuous, Polya's theorem says that $G_{N}\left(
t,F\right) \rightarrow G_{\infty }\left( t,F\right) $ uniformly for all $t$.
\end{itemize}

Therefore, we have that $\left \Vert G_{N}\left( t,\hat{F}\right) -G_{\infty
}\left( t,F\right) \right \Vert \overset{p}{\rightarrow }0$ as $N\rightarrow
\infty $ uniformly over all $t$.

It is very important that $G_{\infty }\left( t,F\right) $ is continuous,
otherwise the bootstrap may not work. For example, the bootstrap is
inconsistent for Manski's maximum score estimator, which is a binary least
absolute deviation estimator%
\begin{equation*}
\underset{\beta }{\min }\sum_{i=1}^{N}\left \vert y_{i}-1\left( x_{i}\beta
\right) \right \vert
\end{equation*}%
However, it turns out that for median (and also quantile) regression%
\begin{equation*}
\underset{\beta }{\min }\sum_{i=1}^{N}\left \vert y_{i}-x_{i}\beta \right
\vert
\end{equation*}%
the bootstrap works.

\subsection{How many bootstraps?}

Obviously, using more bootstrap replications $B$ gives a more accurate
bootstrapped statistic, but in practice this must be weighted against the
computational burden associated with the more replications. How to choose
how many bootstrap replications to use? There exist application-specific
methods for selecting the number of bootstrap replications $B$.\footnote{%
We refer to Andrews and Buchinsky (2000) and Poi (2004).} Cameron and Miller
(2015) write that using \emph{"}$B=400$\emph{\ should be more than adequate
in most settings"}.

\subsection{The residual bootstrap and the Wild bootstrap}

The nonparametric bootstrap can be applied to a wide range of models, but
other bootstrap methods have better properties as they assume a particular
structure. For the cross-sectional linear model with additive iid errors, we
can use the residual bootstrap:

\begin{enumerate}
\item Estimate the original model $y_{i}=\mathbf{x}_{i}\mathbf{\beta }+u_{i}$
by OLS and calculate the residuals $\hat{u}_{i}=y_{i}-\mathbf{x}_{i}\mathbf{%
\hat{\beta}}$. With $N$ observations, we will have $\hat{u}_{1},\hat{u}%
_{2},...,\hat{u}_{N}$ residuals.

\item Draw with replacement from the residuals $\hat{u}_{1},\hat{u}_{2},...,%
\hat{u}_{N}$ to obtain $N$ bootstrap residuals $u_{1}^{\ast },u_{2}^{\ast
},...,u_{N}^{\ast }$.

\item Keep the original $N$ $\mathbf{x}$'s and use the sequence of bootstrap
residuals to create $N$ new dependent variables, that is $y_{i}^{\ast }=%
\mathbf{x}_{i}\mathbf{\hat{\beta}}+u_{i}^{\ast }$. The bootstrap sample is
then $\left( y_{1}^{\ast },\mathbf{x}_{1}\right) ,\left( y_{2}^{\ast },%
\mathbf{x}_{2}\right) ,....,\left( y_{N}^{\ast },\mathbf{x}_{N}\right) $.

\item Use the bootstrap sample to estimate $\mathbf{\beta }_{b}^{\ast }$ as $%
\mathbf{\hat{\beta}}_{b}^{\ast }=\left( \mathbf{X}^{\prime }\mathbf{X}%
\right) ^{-1}\mathbf{X}^{\prime }\mathbf{y}^{\ast }$.

\item Repeat 2)-4) $B$ times to obtain a sequence of bootstrap estimates, $%
\mathbf{\hat{\beta}}_{b}^{\ast }$, $b=1,..,B$.

\item Calculate for example the standard deviation of the $B$ values of $%
\mathbf{\hat{\beta}}_{b}^{\ast }$
\end{enumerate}

By essentially reshuffling the residuals, we effectively break any
relationship between $x$ and $u$. In contrast to this, we have that under
heteroscedasticity, the variance of the error term depends on $x$, $E\left(
u_{i}^{2}|x_{i}\right) =\sigma _{i}^{2}$. Therefore, the so-called Wild
bootstrap replaces the second step above by constructing $u_{1}^{\ast
},u_{2}^{\ast },...,u_{N}^{\ast }$ in a way which preserves the relationship
between $x$ and $u$. Suppose we take each residual and in $50$ percent of
the cases multiplied it by $-1$%
\begin{equation*}
u_{i}^{\ast }=\left\{ 
\begin{array}{cc}
-\hat{u}_{i} & \text{ \ with probability }p=0.5 \\ 
\hat{u}_{i} & \text{ \ with probability }1-p%
\end{array}%
\right.
\end{equation*}%
we will have that for the individual observation $\left( u_{i}^{\ast
}\right) ^{2}=\hat{u}_{i}^{2}$. Moreover, we will have that three out of the
first four moments are the same for the original residuals and the
constructed bootstrap residuals%
\begin{eqnarray*}
E\left( u_{i}^{\ast }\right) &=&0 \\
E\left( \left( u_{i}^{\ast }\right) ^{2}\right) &=&E\left( \hat{u}%
_{i}^{2}\right) \\
E\left( \left( u_{i}^{\ast }\right) ^{4}\right) &=&E\left( \hat{u}%
_{i}^{4}\right)
\end{eqnarray*}

It has been shown that the original and the constructed residuals can at
most have the same moments for three out of the four first moments.
Therefore, the question is which of the three moments we want to hit. Mammen
(1993) shows that if we construct the new residuals as below we can instead
preserve the first three moments, but not the fourth%
\begin{equation*}
u_{i}^{\ast }=\left\{ 
\begin{array}{cc}
\hat{u}_{i}\frac{1-\sqrt{5}}{2} & \text{ \ with probability }p=\frac{1+\sqrt{%
5}}{2\sqrt{5}} \\ 
\hat{u}_{i}\frac{1+\sqrt{5}}{2} & \text{ \ with probability }1-p%
\end{array}%
\right.
\end{equation*}

\subsection{Clustering and the bootstrap}

The bootstrap methods, we have dealt with, assume that errors are drawn
independently. This assumption is violated with time-series data, clustered
data and panel data. The bootstrap procedure for clustered data and panel
data is called the block bootstrap:

\begin{enumerate}
\item Estimate model on original cluster sample to obtain $T_{N}$.

\item The bootstrap sample is created by repeatedly drawing a cluster (with
replacement) from the sample of the $G$ clusters and including all $M_{g}$
observations for the drawn cluster, where the draws continue until the
sample size $N=\sum_{g=1}^{G}M_{g}$ is reached.

\item Use the bootstrap sample to obtain an estimate $T_{N,b}^{\ast }$.

\item Repeat 2)-3) $B$ times to obtain a sequence of bootstrap estimates, $%
T_{N,b}^{\ast }$, $b=1,..,B$.

\item Calculate for example the standard deviation of the $B$ values of $%
T_{N,b}^{\ast }$.
\end{enumerate}

\subsection{Examining the bootstrapped values}

It is a good idea to examine the bootstrapped values to make sure that the
bootstrapped standard errors are not corrupted. This is, in particular,
important when using the clustered bootstrap with few clusters. Therefore,
depict the bootstrapped values using a histogram (or a kernel density).

\begin{itemize}
\item In cases with little variation in a dummy variable (such as an
treatment indicator), some bootstrap samples will have very little variation
and this can give rise to very small standard errors.

\item In cases with fairly high correlation between explanatory variables,
some bootstrap samples will almost have multicollinearity and this can blow
up the bootstrapped standard errors.

\item If the parameter estimates are sensitive to the inclusion of one
cluster, the histogram will show sizable probability mass for outlier
estimates.
\end{itemize}

\begin{thebibliography}{99}
\bibitem{Andrews and Buchinsky (2000)} Andrews, D.W.K. and M. Buchinsky
(2000), "A Three-Step Method for Choosing the Number of Bootstrap
Repetitions", \emph{Econometrica}, Vol. 68, No. 1, pp. 23--51.

\bibitem{Angrist and Pischke (2009)} Angrist, J.D., J.-S. Pischke (2009), 
\emph{"Mostly Harmless Econometrics: An Empiricist's Companion"}, Princeton
University Press.

\bibitem{Bertrand, Duflo and Mullainathan (2004)} Bertrand, Duflo and
Mullainathan (2004), "How Much Should We Trust Differences-in-Differences
Estimates?", \emph{Quarterly Journal of Economics}, vol. 119, pp. 249-275.

\bibitem{Browning et al. (2013)} Browning, M., M. G\o rtz and S.
Leth-Petersen (2013), "Housing Wealth and Consumption: A Micro Panel Study", 
\emph{Economic Journal}, Vol. 123, pp. 401-428.

\bibitem{Cameron and Miller (2015)} Cameron, A.C. and D.L. Miller (2015), "A
Practitioner's Guide to Cluster-Robust Inference", \emph{Journal of Human
Resources}, Vol. 50, No. 2, pp.317-373.

\bibitem{Chesher and Jewitt (1987)} Chesher, A. and I. Jewitt (1987),\ "The
Bias of a Heteroscedasticity Consistent Covariance Matrix Estimator", \emph{%
Econometrica}, Vol. 55, No. 5, pp. 1217-1222.

\bibitem{Efron (1979)} Efron, B (1979), "Bootstrap Methods: Another Look at
the Jackknife", \emph{Annals of Statistics}, Vol. 7, pp. 1-26.

\bibitem{Horowitz (2001)} Horowitz, J.L. (2001), "The Bootstrap", in J.J.
Heckman and E. Leamer (eds.): \emph{"Handbook of Econometrics"}, Vol. 5,
Elsevier Science.

\bibitem{Mackinnon and White (1985)} MacKinnon, J.G., and H. White (1985),
\textquotedblleft Some Heteroskedasticity Consistent Covariance Matrix
Estimators with Improved Finite Sample Properties\textquotedblright , \emph{%
Journal of Econometrics}, Vol. 29, pp. 305-325.

\bibitem{Mammen (1993)} Mammen, E. (1993), "Bootstrap and Wild Bootstrap for
High-dimensional Linear Models", \emph{Annals of Statistics}, Vol. 21, pp.
255-285.

\bibitem{Moulton (1986)} Moulton, B.R. (1986), "Random Group Effects and the
Precision of Regression Estimates", \emph{Journal of Econometrics}, Vol. 32,
No. 2, pp. 385-397.

\bibitem{Poi (2004)} Poi, B.P. (2004), "From the Help Desk: Some
Bootstrapping Techniques", \emph{Stata Journal}, Vol. 4, No. 3, pp. 312-328.

\bibitem{White (1980a)} White, H. (1980a), "A Heteroscedasticity-Consistent
Covariance Matrix Estimator and a Direct Test of Heteroscedasticity", \emph{%
Econometrica}, Vol. 48, No. 4, pp. 817-838.

\bibitem{White (1980b)} White, H. (1980b), "Using Least Squares to
Approximate Unknown Regression Functions", \emph{International Economic
Review}, Vol. 21, No. 1, pp. 149-170.

\bibitem{White (1984)} White, H. (1984), \emph{"Asymptotic Theory for
Econometricians"}, Academic Press, San Diego.
\end{thebibliography}

\end{document}
